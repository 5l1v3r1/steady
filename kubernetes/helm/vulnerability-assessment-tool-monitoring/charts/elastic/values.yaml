# --------------------------------- Kibana -------------------------------------
# Kibana deployment meant to link up with elasticsearch to provide a
# graphical option to analyze log data

kibana:
  # -------------------------- Base declarations -------------------------------
  enabled: true
  # _n_ kibana.replicas
  # _i_
  # Suggestion : ~= elasticsearch cluster members / 2
  replicas: 1

  extraConfigs: {}
  debug: false

  # _n_ kibana.selfAntiAffinity
  # _i_
  # soft anti affinity towards sharing nodes with kibana pods
  # recommended to garantee uptime for services
  selfAntiAffinity: {}
    # soft: true
    # weight: 100

  # _n_ kibana.elasticsearchAffinity
  # _i_
  # soft affinity towards sharing nodes with elasticsearch pods
  # recommended to garantee uptime for services
  elasticsearchAffinity: {}
    # soft: true
    # weight: 100

  # _n_ kibana.podDisruptionBudget
  # _i_
  # Pod disruption budget for kibana deployment makes sure during
  # changes at least {.Values.kibana.minAvailable} kibana pod is available
  podDisruptionBudget: {}
    # minAvailable: 1

  # _n_ kibana.serverHost
  serverHost: "0.0.0.0"

  # _n_ kibana.healthCheckPath
  healthCheckPath: "/app/kibana"

  plugins:
    # _n_ kibana.plugins.enabled
    enabled: false
    # reset: true
    # values:
    #   - logtrail,0.1.31,https://github.com/sivasamyk/logtrail/releases/download/v0.1.31/logtrail-7.3.1-0.1.31.zip

  persistentVolume:
    enabled: false
    storage: {}

  securityContext: {}
    # default security context
    # enabled: true
    # allowPrivilegeEscalation: false
    # runAsUser: 1000

  # ---------------------------- Base images -----------------------------------
  image:
    pullPolicy: "IfNotPresent"
    registry: {}
    registryPort: {}
    # _n_ kibana.image.name
    # _i_
    # Image size : 359MB
    name: "kibana"
    tag: "7.3.1"

    # _n_ kibana.image.tag
    resources: {}
      # limits:
      #   memory: "2G"
      #   cpu: "2000m"
      # requests:
      #   memory: "1G"
      #   cpu: "1000m"

  readinessProbe:
    # _n_ kibana.readinessProbe.enabled
    # _i_
    # Overall time before container is considered unready by k8s :
    # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * successThreshold)
    # = 64s ~= 1min
    enabled: true
    initialDelaySeconds: 15
    failureThreshold: 3
    periodSeconds: 10
    timeoutSeconds: 5

  livenessProbe:
    # _n_ kibana.readinessProbe.enabled
    # _i_
    # Overall time before container is considered unready by k8s :
    # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * successThreshold)
    # = 64s ~= 1min
    enabled: true
    initialDelaySeconds: 30
    failureThreshold: 3
    periodSeconds: 10
    timeoutSeconds: 5

# --------------------------- Elasticsearch ------------------------------------
# Clustered distributed storage for logs
elasticsearch:
  # -------------------------- Base declarations -------------------------------
  # _n_ elasticsearch.debug
  debug: false

  # _n_ elasticsearch.retention
  retention: {}
    # max_size: "100GB"
    # max_age: "180d"
    # min_age: "90d"

  # _n_ elasticsearch.clusterName
  # _d_ Name for the elasticsearch cluster
  clusterName: "elasticsearch"

  # _n_ elasticsearch.nodeGroup
  nodeGroup: "master"

  # _n_ elasticsearch.extraConfigs
  extraConfigs: {}

  # _n_ elasticsearch.replicas
  # _i_
  # Suggestion : >= 3
  replicas: 3

  # _n_ elasticsearch.minimumMasterNodes
  # _i_
  # Minimum quorum required to perform a master election
  # Suggestion : >= card(replicas) / 2
  minimumMasterNodes: 2

  roles:
    # _n_ elasticsearch.roles.master
    # _d_ controls the cluster
    master: "true"
    # _n_ elasticsearch.roles.ingest
    # _d_ accessible endpoints for 'external' resources
    ingest: "true"
    # _n_ elasticsearch.roles.data
    # _d_ storage node
    data: "true"

  # _n_ elasticsearch.esJavaOpts
  # _i_
  # Java heap size for elasticsearch
  # Xmx : maximum heap size
  # Xms : minimum heap size
  # Elasticsearch's recommendation:
  #     Xmx, Xms <= 50% RAM available on server
  #     Xmx, Xms <= threshold JVM uses for compressed objects
  #                 pointers (compressed oops) ~= 32 GB
  #                 can be verified looking at elastic logs
  #     Xmx, Xms <= threshold JVM uses for zero-based compressed
  #                 oops ~= 26Gb on most system
  esJavaOpts: "-Xmx1g -Xms1g"
    # _n_ elasticsearch.networkHost
  networkHost: "0.0.0.0"

  # _n_ elasticsearch.sysctlVmMaxMapCount
  # Since elasticsearch uses mmapfs directory to store its indices
  # increases it to avoid out of memory exceptions
  sysctlVmMaxMapCount: 262144

  # _n_ elasticsearch.selfAntiAffinity
  # _i_
  # soft Anti affinity towards sharing pods with other
  # elasticsearch cluster members
  selfAntiAffinity: 100

  # _n_ elasticsearch.updateStrategy
  # _i_
  # Allows for automated rolling updates
  updateStrategy: RollingUpdate
  # _n_ elasticsearch.podManagementPolicy
  # _i_
  # Allows independent termination and creation of pods
  podManagementPolicy: "Parallel"

  # _n_ elasticsearch.terminationGracePeriodSeconds
  # _i_
  # Allows pods to live a certain period after termination
  terminationGracePeriodSeconds: 120

  # _n_ elasticsearch.clusterHealthCheckParams
  clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

  podDisruptionBudget:
    # _n_ elasticsearch.podDisruptionBudget.maxUnavailable
    maxUnavailable: 1

  # ---------------------------- Base images -----------------------------------
  image:
    # ------------------------- Init Container ---------------------------------
    initContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      # _n_ elasticsearch.image.initContainer.name
      # _i_
      # busybox images for init container to reduce
      # overal resource strain with Image size: 763KB
      name: "busybox"
      tag: "1.31.0"

      # _n_ elasticsearch.image.initContainer.resources
      resources: {}
        # limits:
        #   cpu: "25m"
        #   memory: "150Mi"
        # requests:
        #   cpu: "25m"
        #   memory: "128Mi"

      # _n_ elasticsearch.image.initContainer.securityContext
      securityContext: {}
        # privileged: true
        # runAsUser: 0

    # ------------------------- Main Container ---------------------------------
    mainContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      # _n_ elasticsearch.image.mainContainer.name
      # _i_
      # Image size : 405MB
      name: "elasticsearch"
      tag: "7.3.1"

      # _n_ elasticsearch.image.mainContainer.resources
      resources: {}
        # limits:
        #   cpu: "1000m"
        #   memory: "2Gi"
        # requests:
        #   cpu: "100m"
        #   memory: "1Gi"

      readinessProbe:
        # _n_ elasticsearch.image.mainContainer.readinessProbe.failureThreshold
        # _i_
        # Overall time before container is considered unready by k8s :
        # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * successThreshold)
        # = 64s ~= 1min
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 3
        timeoutSeconds: 5

      # _n_ elasticsearch.image.mainContainer.securityContext
      securityContext: {}
        # capabilities:
        #   drop:
        #   - ALL
        # # readOnlyRootFilesystem: true
        # runAsNonRoot: true
        # runAsUser: 1000


    # ------------------------- Sidecar Container ------------------------------
    sidecarContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      # _n_ elasticsearch.image.sidecarContainer.name
      # _i_
      # Image size : 405MB
      name: "elasticsearch"
      tag: "7.3.1"

      # _n_ elasticsearch.image.sidecarContainer.resources
      resources: {}
        # limits:
        #   cpu: "50m"
        #   memory: "150Mi"
        # requests:
        #   cpu: "25m"
        #   memory: "128Mi"

  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    resources:
      requests:
        storage: "40Gi"
