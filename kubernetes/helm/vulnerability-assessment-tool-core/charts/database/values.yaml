#================================= Postgres ====================================
postgres:
  metrics:
    enabled: true

  masterAntiAffinity: {}
    # soft: true
    # # _n_ postgres.masterAntiAffinity.mastermasterAntiaffinity
    # # _d_ anti affinity between masters
    # mastermasterAntiaffinity:  100
    # # _n_ postgres.masterAntiAffinity.mastermasterAntiaffinity
    # # _d_ anti affinity between masters and slaves
    # masterslaveAntiaffinity: 50

  slaveAntiAffinity: {}
    # soft: true
    # # _n_ postgres.slaveAntiAffinity.weight
    # # _d_ anti affinity between slaves
    # weight: 50

  # _n_ postgres.debug
  # _d_ postgres log level
  # _i_
  # Values in {
  # - DEBUG1..DEBUG5
  # - INFO
  # - NOTICE
  # - WARNING
  # - ERROR
  # - LOG
  # - FATAL
  # - PANIC
  # }
  debug: WARNING

  #================================= Image =====================================
  image:
    # ------------------------- Init Container ---------------------------------
    initContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "postgres"
      # _n_ postgres.image.initContainer.tag
      # _i_
      # Alpine images for init container to reduce
      # overal resource strain with Image size: 28MBs
      tag: "11.5-alpine"

      securityContext:
        runAsUser: 0
        privileged: true

      resources: {}
        # limits:
        #   memory: "35Mi"
        #   cpu: "150m"
        # requests:
        #   memory: "25Mi"
        #   cpu: "100m"

    # ------------------------- Main Container ---------------------------------
    mainContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "postgres"
      # _n_ postgres.image.mainContainer.tag
      # _i_
      # Debian based image
      # Image size : 117MB
      tag: "11.5"

      securityContext:
        runAsUser: 999
        runAsGroup: 999
        privileged: false
        readOnlyRootFilesystem: false

    exporterContainer:
      pullPolicy: "IfNotPresent"
      registry: {}
      registryPort: {}
      name: "wrouesnel/postgres_exporter"
      # _n_ postgres.image.exporterContainer.tag
      # _i_
      # Debian based image
      # Image size : 5.65MB
      tag: "v0.5.1"

  # ---------------------- Base declarations for master ------------------------
  master:
    extraConfigs: {}
    updateStrategy: "RollingUpdate"

    # _n_ postgres.master.replicas
    # _i_
    # Desired master statefulset replicas
    # Suggestion : ~= 1
    #              or multiple but requires strict separation of R/W stream
    #                 due to lack of consistency between masters
    replicas: {}

    resources: {}
      # requests:
      #   memory: "16Gi"
      #   cpu: "16"

    podDisruptionBudget: {}
      # # _n_ postgres.master.podDisruptionBudget.minAvailable
      # # _i_
      # #    Warning :  this won't be applied unless the replicas
      # #               values are >= 2
      # minAvailable: 1

    podPriorityClass:
      # _n_ postgres.master.podPriorityClass.spec
      # _i_
      # If .Values.global.podPriorityClass.enabled and spec is {}
      # defaults to
      #   globalDefault: false
      #   value: 100000
      #   preemptionPolicy: PreemptLowerPriority
      spec: {}

    # Allows pods to live a certain period after termination
    terminationGracePeriodSeconds: 10

    readinessProbe:
      enabled: true
      # _n_ postgres.master.image.readinessProbe.initialDelaySeconds
      # _i_
      # Overall time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 29s ~= .5min
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 3

    livenessProbe:
      enabled: false
      # _n_ postgres.master.image.livenessProbe.initialDelaySeconds
      # _i_
      # Overall time of failed response before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 220s ~= 4min
      initialDelaySeconds: 60
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 20

    # ---------------------- Base resources requests ---------------------------
    # _n_ postgres.master.pvcMountPath
    # _i_
    # This value is injected to postgres using the env variable $PG_DATA
    # and the default value is the one recommended for postgresql imaes
    pvcMountPath: "/var/lib/postgresql/data"
    # _n_ postgres.master.existingClaim
    # _i_
    # Injecting this value ignores all other requests and accessModes input
    # If set to { }, uses the other values
    existingClaim: {}

    accessModes: "ReadWriteOnce"
    requests:
      # _n_ postgres.master.requests.storage
      # _d_ PGDATA size
      # _i_
      # After populating the database with go load and performing a global scan with
      # patchlibanalyzer, the total database weights around 454M, but for actual uses:
      # - 10Gi would be enough for very small test environments
      # - 100Gi recommended for small-medium production environments
      # - 500Gi recommended for big production environments (which is our case)
      storage: "10Gi"

  # ---------------------- Base declarations for slave -------------------------
  slave:
    backoffDuration: 10

    resources: {}
      # requests:
      #   memory: "8Gi"
      #   cpu: "8"
      # limits:
      #   memory: "15Gi"
      #   cpu: "15"

    extraConfigs: {}
    # _n_ postgres.slave.podDisruptionBudget.minAvailable
    # _i_
    # Warning: this won't be applied unless the replicas
    #          values are >= 2
    podDisruptionBudget: {}

    podPriorityClass:
      # _n_ postgres.slave.podPriorityClass.spec
      # _i_
      # If .Values.global.podPriorityClass.enabled and spec is {}
      # defaults to
      #   globalDefault: false
      #   value: 40000
      #   preemptionPolicy: PreemptLowerPriority
      spec: {}

    # Replicas will be dynamically calculated if set to {}
    # # Desired slave statefulset replicas
    # # Suggestion : ~= R * master stateful set replicas
    # #                 with R the desired replication factor
    # # Replicas will be dynamically calculated if set to {}
    # replicas: 3

    updateStrategy: "RollingUpdate"
    terminationGracePeriodSeconds: 10

    readinessProbe:
      enabled: true
      # _n_ postgres.slave.readinessProbe.initialDelaySeconds
      # _i_
      # Overall time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 29s ~= .5min
      initialDelaySeconds: 50
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 3

    livenessProbe:
      enabled: false
      # _n_ postgres.slave.livenessProbe.initialDelaySeconds
      # _i_
      # Overall time of failed response before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 220s ~= 4min
      initialDelaySeconds: 60
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 20

    # ---------------------- Base resources requests ---------------------------
    # _n_ postgres.slave.pvcMountPath
    # _i_
    # This value is injected to postgres using the env variable $PG_DATA
    # and the default value is the one recommended for postgresql images
    pvcMountPath: "/var/lib/postgresql/data"

    accessModes: "ReadWriteOnce"
    requests:
      # _n_ postgres.master.requests.storage
      # _d_ PGDATA size
      # _i_
      # It is recommended to have this value equal to the master pvc claim
      storage: "10Gi"

# ------------------------------ Pgpool cluster --------------------------------
pgpool:

  # ---------------------------- Base declarations -----------------------------
  name: pgpool
  # _n_ pgpool.debug
  # _d_ pgpool log level
  # _i_
  # If set to false this sets CRUNCHY_DEBUG="False" as an env variable
  # else CRUNCHY_DEBUG="True" because this is a crunchy pgpool image
  debug: false
  # _n_ pgpool.replicas
  # _i_
  # Suggestion : ~= master statefulset replicas + slave statefulset replicas
  # dynamically computed if set to {}
  replicas: 3

  podPriorityClass:
    # _n_ pgpool.podPriorityClass.spec
    # _i_
    # If .Values.global.podPriorityClass.enabled and spec is {}
    # defaults to
    #   globalDefault: false
    #   value: 30000
    #   preemptionPolicy: PreemptLowerPriority
    spec: {}

  # _n_ pgpool.updateStrategy
  # _i_
  # Allows for automated rolling updates
  updateStrategy: "RollingUpdate"

  podDisruptionBudget: {}
    # # _n_ pgpool.podDisruptionBudget.dynamic
    # # _i_
    # #    Warning :  this won't be applied unless the replicas
    # #               values are >= 2
    # # Pod disruption budget can be set dynamically or statically
    # # dynamic: sum ( masters + replication factor * master )
    # dynamic: true
    # # minAvailable: 1

  antiAffinity: {}
    # soft: true
    # # _n_ pgpool.antiAffinity.selfAntiAffinity
    # # _d_ antiAffinity towards own pods
    # selfAntiAffinity: 50

  affinity: {}
    # soft: true
    # # _n_ pgpool.affinity.poolreplicaAffinity
    # # _i_
    # # affinity towards being close to postgres replica nodes
    # poolreplicaAffinity: 50
    # # _n_ pgpool.affinity.poolmasterAffinity
    # # _i_
    # # affinity towards being close to postgres master nodes
    # # preference higher than replica nodes because write functions
    # # are only available on master nodes
    # poolmasterAffinity: 70

  loadBalanceMode: off
  failOverOnBackendError: off
  numInitChildren: 100

  maxPool: 10
  clientIdleLimit: 920
  connectionLifeTime: 500

  # Allows pods to live a certain period after termination
  terminationGracePeriodSeconds: 15
  useWatchDog: on
  watchDog:
    interval: 15

  healthCheck:
    timeout: 0
    period: 30
    maxRetries: 5
    retryDelay: 5

  image:
    pullPolicy: "IfNotPresent"
    registry: {}
    registryPort: {}
    name: "crunchydata/crunchy-pgpool"
    tag: "centos7-11.4-2.4.1"

    securityContext:
      runAsUser: 999
      runAsGroup: 999
      privileged: false
      readOnlyRootFilesystem: false

    resources: {}
      # limits:
      #   memory: "2Gi"
      #   cpu: "2000m"
      # requests:
      #   memory: "1Gi"
      #   cpu: "1000m"

    readinessProbe:
      # _n_ pgpool.image.readinessProbe.enabled
      # _i_
      # Overall time of failed response before container is considered ready by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 665s ~= 11min
      enabled: true
      initialDelaySeconds: 35
      periodSeconds: 10
      timeoutSeconds: 10
      failureThreshold: 3

    livenessProbe:
      # _n_ pgpool.image.livenessProbe.enabled
      # _i_
      # Overall time before container is killed by k8s :
      # initialDelaySeconds + ((periodSeconds + timeoutSeconds) * failureThreshold)
      # = 720s ~= 12min
      enabled: true
      initialDelaySeconds: 35
      periodSeconds: 30
      timeoutSeconds: 10
      failureThreshold: 3
